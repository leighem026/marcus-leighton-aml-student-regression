{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mid-Training Capstone Assesment\n",
    "York Solutions | AML Best Buy Cohort Cohort 2023-2024\n",
    "Technical Training Consultant: Marcus Leighton\n",
    "\n",
    "\n",
    "Regression Analysis on Student Grades: \n",
    "\n",
    "An educational consultancy is interested in understanding factors that influence student performance. They are looking for a regression model to predict the final grades of students based on a vast array of personal and socio-economic facotrs. Accurate predictions can help in identiying students who might need additional support. The data is provided in a CSV format exported from excel, but they would like this transferred to a Postgres database. Additionally, they require a python function that will allow them to add a new student to the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests & Requirements:\n",
    "\n",
    "- Perform Exploratory Data Analysis (EDA) with visualizations to assist in the feature selection and engineering\n",
    "- Convert the CSV to a Postgres database\n",
    "- Create a Python Function to easily add new students to the Postgres Database, Ensure the database schema has approprate datatypes. \n",
    "- Selection and/or engineer the feature set you will use for your model \n",
    "- Select and train a model to predict the final grade of a student\n",
    "- Tune the model to get the best results\n",
    "- Generate valid metrics to evaluate your model\n",
    "- ALL code shall be written in Python or SQL \n",
    "- ALL code shall be managed via 'git'\n",
    "- Only Libraries inherent to Python or listed below can be used\n",
    "- The Postgres Database should be managed via Docker\n",
    "- Only data from the given dataset will be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowed Dependencies:\n",
    "\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- scipy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- seaborn\n",
    "- psycopg2\n",
    "- sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Requests (Bonus):\n",
    "\n",
    "- Create another model that predicts final grades without using any of the previous grades\n",
    "- Write a full Data Analysis report on the statistics gleaned from the dataset\n",
    "- Include documentation used to plan out this project and its timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 - PROJECT SETUP\n",
    "\n",
    "This block of notes is meant to illustrate the steps required to setup the project. These are terminal commands necessary to create a new repository initialize project structure. I only include these steps to demonstrate my knowledge in the setup process, assuming that under any other circumstances, the required libraries MAY NOT be readily available in the user's Python programming language and applications. \n",
    "\n",
    "# Clone the Repository\n",
    "git clone https://github.com/myusername/myreponame.git   <-- change username and reponame to reflect my own\n",
    "\n",
    "# Navigate to the Project Directory\n",
    "cd your-repo-name    <-- change reponame to reflect my own\n",
    "\n",
    "# Create Virutal Environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate Virtual Enviroment\n",
    "source venv/bin/activate\n",
    "\n",
    "# Install the required liraries, \"Allowed Dependicies\"\n",
    "pip install scikit-learn numpy scipy pandas matplotlib seaborn psycopg2 sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating Git: \n",
    "\n",
    "git add --all\n",
    "\n",
    "git commit -m \"some commit message\"\n",
    "\n",
    "git push\n",
    "\n",
    "            git config --global --edit   # we reset author, we set it once, and never touch\n",
    "\n",
    "            git commit --amend --reset-author\n",
    "\n",
    "git status\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2 - ETL & EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "Exploratory Data Analysis is used to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers one needs, making it easier for data scientists to discover patterns, spot anomalies, test hypotheses, and/or check assumptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libaries, metrics, and analysis tools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # Learned that there were no missing values in the data, so not needed. \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load the Dataset with Semicolon Separator and No Header  \n",
    "df = pd.read_csv('data.csv', sep=';', header=None, skiprows=1)\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset schema with data types\n",
    "schema = {\n",
    "    'school': 'binary',\n",
    "    'sex': 'binary',\n",
    "    'age': 'numeric',\n",
    "    'address': 'binary',\n",
    "    'famsize': 'binary',\n",
    "    'Pstatus': 'binary',\n",
    "    'Medu': 'numeric',\n",
    "    'Fedu': 'numeric',\n",
    "    'Mjob': 'nominal',   # 'nonimal' is a categorical-type with no inherent order or structure\n",
    "    'Fjob': 'nominal',\n",
    "    'reason': 'nominal',\n",
    "    'guardian': 'nominal',\n",
    "    'traveltime': 'numeric',\n",
    "    'studytime': 'numeric',\n",
    "    'failures': 'numeric',\n",
    "    'schoolsup': 'binary',\n",
    "    'famsup': 'binary',\n",
    "    'paid': 'binary',\n",
    "    'activities': 'binary',\n",
    "    'nursery': 'binary',\n",
    "    'higher': 'binary',\n",
    "    'internet': 'binary',\n",
    "    'romantic': 'binary',\n",
    "    'famrel': 'numeric',\n",
    "    'freetime': 'numeric',\n",
    "    'goout': 'numeric',\n",
    "    'Dalc': 'numeric',\n",
    "    'Walc': 'numeric',\n",
    "    'health': 'numeric',\n",
    "    'absences': 'numeric',\n",
    "    'G1': 'numeric',\n",
    "    'G2': 'numeric',\n",
    "    'G3': 'numeric'\n",
    "}\n",
    "\n",
    "# Manually specify schema information for each column\n",
    "schema_info = {\n",
    "    'school': {'description': \"student's school\", 'options': ['GP', 'MS']},\n",
    "    'sex': {'description': \"student's sex\", 'options': ['F', 'M']},\n",
    "    'age': {'description': \"student's age (numeric)\", 'min': 15, 'max': 22},\n",
    "    'address': {'description': \"student's home address type\", 'options': ['U', 'R']},\n",
    "    'famsize': {'description': \"family size\", 'options': ['LE3', 'GT3']},\n",
    "    'Pstatus': {'description': \"parent's cohabitation status\", 'options': ['T', 'A']},\n",
    "    'Medu': {'description': \"mother's education\", 'options': [0, 1, 2, 3, 4]},\n",
    "    'Fedu': {'description': \"father's education\", 'options': [0, 1, 2, 3, 4]},\n",
    "    'Mjob': {'description': \"mother's job\", 'options': ['teacher', 'health', 'services', 'at_home', 'other']},\n",
    "    'Fjob': {'description': \"father's job\", 'options': ['teacher', 'health', 'services', 'at_home', 'other']},\n",
    "    'reason': {'description': \"reason to choose this school\", 'options': ['home', 'reputation', 'course', 'other']},\n",
    "    'guardian': {'description': \"student's guardian\", 'options': ['mother', 'father', 'other']},\n",
    "    'traveltime': {'description': \"home to school travel time\", 'options': [1, 2, 3, 4]},\n",
    "    'studytime': {'description': \"weekly study time\", 'options': [1, 2, 3, 4]},\n",
    "    'failures': {'description': \"number of past class failures\", 'min': 1, 'max': 4},\n",
    "    'schoolsup': {'description': \"extra educational support\", 'options': ['yes', 'no']},\n",
    "    'famsup': {'description': \"family educational support\", 'options': ['yes', 'no']},\n",
    "    'paid': {'description': \"extra paid classes within the course subject\", 'options': ['yes', 'no']},\n",
    "    'activities': {'description': \"extra-curricular activities\", 'options': ['yes', 'no']},\n",
    "    'nursery': {'description': \"attended nursery school\", 'options': ['yes', 'no']},\n",
    "    'higher': {'description': \"wants to take higher education\", 'options': ['yes', 'no']},\n",
    "    'internet': {'description': \"Internet access at home\", 'options': ['yes', 'no']},\n",
    "    'romantic': {'description': \"with a romantic relationship\", 'options': ['yes', 'no']},\n",
    "    'famrel': {'description': \"quality of family relationships\", 'min': 1, 'max': 5},\n",
    "    'freetime': {'description': \"free time after school\", 'min': 1, 'max': 5},\n",
    "    'goout': {'description': \"going out with friends\", 'min': 1, 'max': 5},\n",
    "    'Dalc': {'description': \"workday alcohol consumption\", 'min': 1, 'max': 5},\n",
    "    'Walc': {'description': \"weekend alcohol consumption\", 'min': 1, 'max': 5},\n",
    "    'health': {'description': \"current health status\", 'min': 1, 'max': 5},\n",
    "    'absences': {'description': \"number of school absences\", 'min': 0, 'max': 93},\n",
    "    'G1': {'description': \"first period grade\", 'min': 0, 'max': 20},\n",
    "    'G2': {'description': \"second period grade\", 'min': 0, 'max': 20},\n",
    "    'G3': {'description': \"final grade (output target)\", 'min': 0, 'max': 20}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df.columns = list(schema.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the schema to convert binary and nominal [categorial] columns to the specified data types\n",
    "for column, conversion_func in schema.items():\n",
    "    if column in schema_info:\n",
    "        # Apply custom conversion for specific columns with additional information\n",
    "        if schema[column] == 'binary':\n",
    "            # Convert binary variables with non-numeric options to 0s and 1s\n",
    "            conversion_options = {opt: idx for idx, opt in enumerate(schema_info[column]['options'])}\n",
    "            df[column] = df[column].apply(lambda x: conversion_options[x] if x in conversion_options else None)\n",
    "        elif schema[column] == 'nominal':\n",
    "            df[column] = df[column].apply(lambda x: schema_info[column]['options'].index(x) if x in schema_info[column]['options'] else None)\n",
    "    else:\n",
    "        # Apply regular conversion for other columns\n",
    "        df[column] = df[column].apply(conversion_func)\n",
    "\n",
    "# Display the first few rows of the DataFrame to inspect the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after applying the schema:\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values: \\n\", missing_values)\n",
    "\n",
    "# Handle Missing Values\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_columns = df.select_dtypes(include='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scatter plots between pairs of numeric variables using Pair Plots\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.pairplot(df[numeric_columns])\n",
    "plt.title(\"Pair Plot - Scatter Plots between Numeric Variables\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation between numeric variables using a Correlation Matrix Heatmap with improved readability\n",
    "plt.figure(figsize=(20, 16))\n",
    "heatmap = sns.heatmap(df[numeric_columns].corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, square=True, cbar_kws={\"shrink\": 0.75})\n",
    "plt.title(\"Correlation Matrix Heatmap - Numeric Variables\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Adjust the colorbar position (debugging for improved readability)\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.set_ticks([-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1])  # Tick positions can be customized\n",
    "cbar.set_ticklabels([-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1])  # Tick labels can be customized\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: Uploading the database to Postgres, \n",
    "\n",
    "This step should generally be done before ANY model pre-processing occurs. The reasoning behind this is that it helps maintain data integrity and consitency, facilitating data management. Storing the data in a centralized database will make it easier to collaborate with other team members (including version control). All of these will create a a modular and organized workflow with scalability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Docker container for Postgres\n",
    "\n",
    "docker container create --name student-grades -p 5432:5432 -e POSTGRES_PASSWORD=password postgres\n",
    "\n",
    "docker container start student-grades \n",
    "\n",
    "cd Documents\n",
    "\n",
    "git clone https://github.com/leighem026/marcus-leighton-aml-student-regression.git\n",
    "\n",
    "The above script are terminal commands, which create a detached Docker container names 'postgres-container' with the password as 'password' for the default Postgres user. PORT 5432 on the local machine is mapped to port 5432 on the container, allowing connection to the Postgres database. DASH-D, rus in detached mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection URL\n",
    "db_url = 'postgresql://postgres:password@localhost:5432/postgres'\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Create the necessary table with the defined schema\n",
    "df.to_sql('student_grades', engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python Function to add New Student information to the database\n",
    "def add_student_to_database(engine, data):\n",
    "    \"\"\"\n",
    "    Adds new student information to the Postgres database.\n",
    "\n",
    "    Parameters:\n",
    "    - engine: SQLAlchemy engine for database connection\n",
    "    - data: Pandas DataFrame containing new student information\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        data.to_sql('student_grades', connection, index=False, if_exists='append')  # the CSV file is called dats, double-check 'student-grades'\n",
    "\n",
    "# Here is an example of how a user would upload new_student_data to the database:\n",
    "# new_student_data = pd.DataFrame({'school': ..., 'sex': ..., ...})\n",
    "# add_student_to_database(engine, new_student_data)\n",
    "        \n",
    "# The 'add_student_to_database' function takes an SQLAlchemy Engine ('engine') and a Pandas Dataframe ('data') as parameters, the above code\n",
    "# uses a 'with' block to ensure that the connnection is properly closed after use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Note: Given the assignment requirements, there is no mention for a Postgres-database-centric approach other than simply being able to upload the database and being able to add new student data. Since the CSV-data and the subsequent Pandas-dataframe for this project are relatively small,compared to larger datasets which could contain hundreds of columns or thousands of rows, I will forego establishing a connection to the Postgres Database moving forward. The small dataset and the pandas dataframe will help simplify the process without the need to establish a connection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 4: Pipeline Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the columns to keep\n",
    "columns_to_keep = ['Mjob', 'Fjob', 'reason', 'guardian']\n",
    "\n",
    "# One-hot encode nominal [categorical] variables\n",
    "df_encoded = pd.get_dummies(df, columns=columns_to_keep, drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate columns from the one-hot encoded DataFrame\n",
    "df_encoded = df_encoded.loc[:, ~df_encoded.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
    "df_final = pd.concat([df, df_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "df_final.fillna(df_final.mean(), inplace=True)\n",
    "\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric columns for later use\n",
    "numeric_columns = df_final.select_dtypes(include=['float64', 'int64']).columns  # moving forward, I will need to change all 'df' to 'df_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_final[numeric_columns] = minmax_scaler.fit_transform(df_final[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_scaler = StandardScaler()\n",
    "df_final[numeric_columns] = zscore_scaler.fit_transform(df_final[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types and number of columns in df_final\n",
    "print(df_final.dtypes)\n",
    "print(\"Number of Columns:\", len(df_final.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final[['Medu', 'Fedu', 'Pstatus', 'Mjob', 'Fjob']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define selected features based on your interaction conditions\n",
    "selected_features = [\n",
    "    # Original data variables\n",
    "    'school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
    "    'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures',\n",
    "    'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet',\n",
    "    'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences',\n",
    "    'G1', 'G2', 'G3',  # Original data variables\n",
    "\n",
    "    # Engineered features One Hot Encoding,\n",
    "    'Mjob_health', 'Mjob_services', 'Mjob_teacher', 'Mjob_at_home', 'Mjob_other',\n",
    "    'Fjob_health', 'Fjob_services', 'Fjob_teacher', 'Fjob_at_home', 'Fjob_other',\n",
    "    'reason_reputation', 'reason_course', 'reason_other', 'guardian_father', 'guardian_other',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 5: TRAIN & TEST the Selected Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target variable (y)\n",
    "X = df_final[selected_features]\n",
    "y = df_final['G3']  # the target is the student's final grade\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Analysis\n",
    "random_forest_pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(RandomForestRegressor())),\n",
    "    ('random_forest', RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "random_forest_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "y_pred_rf = random_forest_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Random Forest model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Mean Squared Error: {mse_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Analysis with Hyperparameter Tuning\n",
    "param_grid_rf = {\n",
    "    'random_forest__n_estimators': [50, 100, 150],  # Example values, you can modify these\n",
    "    'random_forest__max_depth': [None, 10, 20],  # Example values, you can modify these\n",
    "    'random_forest__min_samples_split': [2, 5, 10],  # Example values, you can modify these\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with GridSearchCV\n",
    "random_forest_pipeline_tuned = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(RandomForestRegressor())),\n",
    "    ('random_forest', RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search_rf = GridSearchCV(random_forest_pipeline_tuned, param_grid=param_grid_rf, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "print(\"Best Parameters for Random Forest:\", best_params_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set using the best model\n",
    "y_pred_rf_tuned = grid_search_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model with hyperparameter tuning\n",
    "mse_rf_tuned = mean_squared_error(y_test, y_pred_rf_tuned)\n",
    "print(f\"Tuned Random Forest Mean Squared Error: {mse_rf_tuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Analysis\n",
    "grad_boost_pipeline = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(GradientBoostingRegressor())),\n",
    "    ('gradient_boosting', GradientBoostingRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "grad_boost_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred_gb = grad_boost_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "print(f\"Gradient Boosting Mean Squared Error: {mse_gb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Gradient Boosting Analysis with Hyperparameter Tuning\n",
    "param_grid_gb = {\n",
    "    'gradient_boosting__n_estimators': [50, 100, 150],  # Example values, you can modify these\n",
    "    'gradient_boosting__learning_rate': [0.01, 0.1, 0.2],  # Example values, you can modify these\n",
    "    'gradient_boosting__max_depth': [3, 5, 7],  # Example values, you can modify these\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with GridSearchCV\n",
    "grad_boost_pipeline_tuned = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(GradientBoostingRegressor())),\n",
    "    ('gradient_boosting', GradientBoostingRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search_gb = GridSearchCV(grad_boost_pipeline_tuned, param_grid=param_grid_gb, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params_gb = grid_search_gb.best_params_\n",
    "print(\"Best Parameters for Gradient Boosting:\", best_params_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set using the best model\n",
    "y_pred_gb_tuned = grid_search_gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Gradient Boosting model with hyperparameter tuning\n",
    "mse_gb_tuned = mean_squared_error(y_test, y_pred_gb_tuned)\n",
    "print(f\"Tuned Gradient Boosting Mean Squared Error: {mse_gb_tuned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Plot between Random Forest and Gradient Boosting models:\n",
    "\n",
    "# MSE values\n",
    "mse_values = [mse_rf_tuned, mse_gb_tuned]\n",
    "\n",
    "# Model names\n",
    "model_names = ['Random Forest', 'Gradient Boosting']\n",
    "\n",
    "# Plotting\n",
    "plt.bar(model_names, mse_values, color=['blue', 'orange'])\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Comparison of Model Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions Scatter Plot:\n",
    "plt.scatter(y_test, y_pred_rf_tuned, label='Random Forest', alpha=0.5)\n",
    "plt.scatter(y_test, y_pred_gb_tuned, label='Gradient Boosting', alpha=0.5)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.legend()\n",
    "plt.title('Predictions Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals Plot:\n",
    "\n",
    "residuals_rf = y_test - y_pred_rf_tuned\n",
    "residuals_gb = y_test - y_pred_gb_tuned\n",
    "\n",
    "plt.scatter(y_test, residuals_rf, label='Random Forest', alpha=0.5)\n",
    "plt.scatter(y_test, residuals_gb, label='Gradient Boosting', alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend()\n",
    "plt.title('Residuals Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ALL code shall be managed via 'git'\n",
    "- The Postgres Database should be managed via Docker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
